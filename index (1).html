<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Computer Vision</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/mobilenet@2.1.0/dist/mobilenet.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd@2.2.2/dist/coco-ssd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/deeplab@0.2.1/dist/deeplab.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/knn-classifier@1.2.4/dist/knn-classifier.min.js"></script>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
            overflow: hidden;
        }

        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            text-align: center;
        }

        .header h1 {
            font-size: 2.5rem;
            margin-bottom: 10px;
            font-weight: 300;
        }

        .header p {
            font-size: 1.1rem;
            opacity: 0.9;
        }

        .main-content {
            padding: 30px;
        }

        .upload-section {
            text-align: center;
            margin-bottom: 40px;
            padding: 30px;
            border: 3px dashed #ddd;
            border-radius: 15px;
            background: #fafafa;
            transition: all 0.3s ease;
        }

        .upload-section:hover {
            border-color: #667eea;
            background: #f0f4ff;
        }

        .upload-btn {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            padding: 15px 30px;
            border-radius: 25px;
            font-size: 1.1rem;
            cursor: pointer;
            transition: transform 0.3s ease;
            margin: 10px;
        }

        .upload-btn:hover {
            transform: translateY(-2px);
        }

        .file-input {
            display: none;
        }

        .cv-tasks {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 30px;
            margin-top: 30px;
        }

        .task-card {
            background: white;
            border-radius: 15px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
            overflow: hidden;
            transition: transform 0.3s ease;
        }

        .task-card:hover {
            transform: translateY(-5px);
        }

        .task-header {
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }

        .task-header h3 {
            font-size: 1.3rem;
            margin-bottom: 5px;
        }

        .task-header p {
            font-size: 0.9rem;
            opacity: 0.9;
        }

        .task-content {
            padding: 20px;
        }

        .canvas-container {
            position: relative;
            margin-bottom: 20px;
            text-align: center;
        }

        canvas {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.1);
        }

        .results {
            background: #f8f9fa;
            border-radius: 10px;
            padding: 15px;
            margin-top: 15px;
            min-height: 100px;
        }

        .results h4 {
            color: #333;
            margin-bottom: 10px;
            font-size: 1.1rem;
        }

        .prediction {
            background: white;
            border-radius: 8px;
            padding: 10px;
            margin: 5px 0;
            border-left: 4px solid #667eea;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .confidence {
            background: #667eea;
            color: white;
            padding: 3px 8px;
            border-radius: 12px;
            font-size: 0.8rem;
            font-weight: bold;
        }

        .loading {
            text-align: center;
            color: #666;
            font-style: italic;
            padding: 20px;
        }

        .error {
            background: #fee;
            color: #c33;
            padding: 15px;
            border-radius: 8px;
            margin: 10px 0;
            border-left: 4px solid #c33;
        }

        .status-indicator {
            display: inline-block;
            width: 10px;
            height: 10px;
            border-radius: 50%;
            margin-right: 8px;
        }

        .status-loading { 
            background: #ffa500; 
            animation: pulse 2s infinite;
        }
        .status-ready { background: #28a745; }
        .status-error { background: #dc3545; }
        
        @keyframes pulse {
            0% { opacity: 1; }
            50% { opacity: 0.5; }
            100% { opacity: 1; }
        }

        .model-status {
            background: #f8f9fa;
            border-radius: 10px;
            padding: 20px;
            margin-bottom: 30px;
        }

        .model-status h3 {
            margin-bottom: 15px;
            color: #333;
        }

        .status-item {
            display: flex;
            align-items: center;
            margin: 8px 0;
            font-size: 0.9rem;
        }

        .attention-map {
            position: relative;
            display: inline-block;
        }

        .attention-overlay {
            position: absolute;
            top: 0;
            left: 0;
            opacity: 0.6;
            pointer-events: none;
        }

        .cnn-layer {
            background: #f8f9fa;
            border: 2px solid #ddd;
            border-radius: 8px;
            padding: 10px;
            margin: 5px;
            text-align: center;
            display: inline-block;
            min-width: 80px;
        }

        .cnn-arrow {
            display: inline-block;
            margin: 0 10px;
            color: #667eea;
            font-size: 1.2rem;
        }

        .gan-progress {
            background: #f0f0f0;
            border-radius: 10px;
            height: 20px;
            margin: 10px 0;
            overflow: hidden;
        }

        .gan-progress-bar {
            background: linear-gradient(90deg, #667eea, #764ba2);
            height: 100%;
            width: 0%;
            transition: width 0.3s ease;
        }

        /* Feature Detection Styles */
        .feature-controls {
            display: flex;
            gap: 10px;
            margin: 15px 0;
            flex-wrap: wrap;
        }

        .feature-btn {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            border: none;
            padding: 8px 16px;
            border-radius: 20px;
            cursor: pointer;
            font-size: 0.9rem;
            transition: all 0.3s ease;
        }

        .feature-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
        }

        .feature-btn.active {
            background: linear-gradient(135deg, #28a745, #20c997);
            transform: scale(1.05);
        }

        .feature-stats {
            background: #f8f9fa;
            border-radius: 8px;
            padding: 15px;
            margin: 15px 0;
        }

        .stat-item {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 8px 0;
            padding: 5px 0;
            border-bottom: 1px solid #e9ecef;
        }

        .stat-item:last-child {
            border-bottom: none;
        }

        .stat-item span:first-child {
            font-weight: 500;
            color: #495057;
        }

        .stat-item span:last-child {
            color: #667eea;
            font-weight: bold;
        }

        /* Style Transfer Styles */
        .style-transfer-controls {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            gap: 15px;
            margin: 15px 0;
        }

        .style-option {
            background: white;
            border: 2px solid #e9ecef;
            border-radius: 10px;
            padding: 10px;
            cursor: pointer;
            transition: all 0.3s ease;
            text-align: center;
        }

        .style-option:hover {
            border-color: #667eea;
            transform: translateY(-2px);
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.2);
        }

        .style-option.selected {
            border-color: #28a745;
            background: #f8fff9;
            transform: scale(1.05);
        }

        .style-preview {
            width: 100%;
            height: 60px;
            border-radius: 8px;
            margin-bottom: 8px;
            background: linear-gradient(45deg, #667eea, #764ba2);
        }

        .style-option p {
            margin: 0;
            font-weight: 500;
            color: #495057;
        }
            transition: width 0.3s ease;
        }

        .transfer-learning-demo {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 15px 0;
        }

        .tl-section {
            background: white;
            border: 2px solid #eee;
            border-radius: 8px;
            padding: 15px;
        }

        .tl-section h5 {
            color: #667eea;
            margin-bottom: 10px;
        }

        .layer-visualization {
            display: flex;
            flex-wrap: wrap;
            gap: 5px;
            margin: 10px 0;
        }

        .layer-block {
            width: 20px;
            height: 20px;
            background: #ddd;
            border-radius: 3px;
            transition: background-color 0.3s ease;
        }

        .layer-block.frozen {
            background: #6c757d;
        }

        .layer-block.trainable {
            background: #28a745;
        }

        .layer-block.active {
            background: #667eea;
            animation: layer-pulse 1s infinite;
        }

        @keyframes layer-pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }

        .latent-space {
            background: radial-gradient(circle, #667eea 0%, #764ba2 100%);
            border-radius: 50%;
            width: 100px;
            height: 100px;
            margin: 20px auto;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-weight: bold;
        }

        .generative-controls {
            display: flex;
            gap: 10px;
            margin: 15px 0;
            flex-wrap: wrap;
        }

        .control-group {
            flex: 1;
            min-width: 120px;
        }

        .control-group label {
            display: block;
            font-size: 0.9rem;
            color: #666;
            margin-bottom: 5px;
        }

        .control-group input[type="range"] {
            width: 100%;
        }

        .mini-canvas {
            border: 1px solid #ddd;
            border-radius: 5px;
            margin: 5px;
        }

        .pillow-controls {
            display: flex;
            gap: 10px;
            margin: 15px 0;
            flex-wrap: wrap;
        }

        .pillow-btn {
            background: #28a745;
            color: white;
            border: none;
            padding: 8px 16px;
            border-radius: 15px;
            font-size: 0.9rem;
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .pillow-btn:hover {
            background: #218838;
            transform: translateY(-1px);
        }

        .pillow-btn:disabled {
            opacity: 0.6;
            cursor: not-allowed;
            transform: none;
        }

        .stats {
            background: #e3f2fd;
            border-radius: 8px;
            padding: 15px;
            margin: 10px 0;
        }

        .stats h5 {
            color: #1565c0;
            margin-bottom: 8px;
        }

        .stat-item {
            display: flex;
            justify-content: space-between;
            margin: 5px 0;
            font-size: 0.9rem;
        }

        .detection {
            background: white;
            border-radius: 8px;
            padding: 10px;
            margin: 5px 0;
            border-left: 4px solid #667eea;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        @media (max-width: 768px) {
            .cv-tasks {
                grid-template-columns: 1fr;
            }
            
            .header h1 {
                font-size: 2rem;
            }
            
            .main-content {
                padding: 20px;
            }

            .transfer-learning-demo {
                grid-template-columns: 1fr;
            }

            .generative-controls {
                flex-direction: column;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>Computer Vision</h1>
            <p>TensorFlow.js ‚Ä¢ YOLO Detection ‚Ä¢ Pillow Processing ‚Ä¢ Advanced ML ‚Ä¢ CNN ‚Ä¢ Transfer Learning</p>
        </div>

        <div class="main-content">
            <!-- Model Loading Status -->
            <div class="model-status">
                <h3>Model Loading Status</h3>
                <p><small>Advanced neural networks loading in parallel. Faster on subsequent visits due to browser caching.</small></p>
                
                <!-- Progress bar -->
                <div style="background: #f0f0f0; border-radius: 10px; height: 8px; margin: 15px 0; overflow: hidden;">
                    <div id="progress-bar" style="background: linear-gradient(90deg, #667eea, #764ba2); height: 100%; width: 0%; transition: width 0.3s ease;"></div>
                </div>
                <div id="progress-text" style="text-align: center; font-size: 0.9em; color: #666; margin-bottom: 15px;">Starting download...</div>
                
                <div class="status-item">
                    <span class="status-indicator status-loading" id="mobilenet-status"></span>
                    <span id="mobilenet-text">MobileNet (Classification) - Loading...</span>
                </div>
                <div class="status-item">
                    <span class="status-indicator status-loading" id="cocossd-status"></span>
                    <span id="cocossd-text">COCO-SSD (Object Detection) - Loading...</span>
                </div>
                <div class="status-item">
                    <span class="status-indicator status-loading" id="deeplab-status"></span>
                    <span id="deeplab-text">DeepLab (Segmentation) - Loading...</span>
                </div>
                <div class="status-item">
                    <span class="status-indicator status-loading" id="cnn-status"></span>
                    <span id="cnn-text">CNN Architecture - Loading...</span>
                </div>
                <div class="status-item">
                    <span class="status-indicator status-loading" id="transfer-status"></span>
                    <span id="transfer-text">Transfer Learning Models - Loading...</span>
                </div>
            </div>

            <!-- Upload Section -->
            <div class="upload-section">
                <h3>Upload an Image</h3>
                <p>Select an image to analyze with advanced computer vision models</p>
                <button class="upload-btn" onclick="document.getElementById('imageInput').click()">
                    Choose Image
                </button>
                <button class="upload-btn" onclick="loadSampleImage()">
                    Load Sample
                </button>
                <button class="upload-btn" onclick="testModels()">
                    Test Models
                </button>

                <button class="upload-btn" onclick="demonstrateDetection()">
                    Demo Detection
                </button>
                <input type="file" id="imageInput" class="file-input" accept="image/*" onchange="loadImage(event)">
            </div>

            <!-- Computer Vision Tasks -->
            <div class="cv-tasks">
                
                <!-- YOLO Object Detection -->
                <div class="task-card">
                    <div class="task-header">
                        <h3>üéØ YOLO Object Detection</h3>
                        <p>State-of-the-art object detection with bounding boxes</p>
                    </div>
                    <div class="task-content">
                        <div class="canvas-container">
                            <canvas id="yoloCanvas" width="300" height="300"></canvas>
                        </div>
                        <button class="upload-btn" id="yoloBtn" onclick="runYOLODetection()" disabled>
                            Run YOLO Detection
                        </button>
                        <div class="results" id="yoloResults">
                            <h4>YOLO Detection Results</h4>
                            <p>Upload an image to see object detection results</p>
                        </div>
                    </div>
                </div>

                <!-- Pillow Image Processing -->
                <div class="task-card">
                    <div class="task-header">
                        <h3>üñºÔ∏è Pillow Image Processing</h3>
                        <p>Advanced image manipulation and enhancement</p>
                    </div>
                    <div class="task-content">
                        <div class="canvas-container">
                            <canvas id="pillowCanvas" width="300" height="300"></canvas>
                        </div>
                        <div class="pillow-controls">
                            <button class="pillow-btn" onclick="processImagePillow('enhance')" disabled id="enhanceBtn">Enhance</button>
                            <button class="pillow-btn" onclick="processImagePillow('blur')" disabled id="blurBtn">Blur</button>
                            <button class="pillow-btn" onclick="processImagePillow('sharpen')" disabled id="sharpenBtn">Sharpen</button>
                            <button class="pillow-btn" onclick="processImagePillow('edge')" disabled id="edgeBtn">Edge Detect</button>
                            <button class="pillow-btn" onclick="processImagePillow('vintage')" disabled id="vintageBtn">Vintage</button>
                            <button class="pillow-btn" onclick="processImagePillow('grayscale')" disabled id="grayscaleBtn">Grayscale</button>
                        </div>
                        <div class="results" id="pillowResults">
                            <h4>Image Processing Results</h4>
                            <p>Select an operation to enhance your image</p>
                        </div>
                    </div>
                </div>

                <!-- Combined Analysis -->
                <div class="task-card">
                    <div class="task-header">
                        <h3>üî¨ Combined Analysis</h3>
                        <p>YOLO detection + Pillow processing in one workflow</p>
                    </div>
                    <div class="task-content">
                        <div class="canvas-container">
                            <canvas id="combinedCanvas" width="300" height="300"></canvas>
                        </div>
                        <button class="upload-btn" id="combinedBtn" onclick="runCombinedAnalysis()" disabled>
                            Run Combined Analysis
                        </button>
                        <div class="results" id="combinedResults">
                            <h4>Combined Analysis Results</h4>
                            <p>Advanced analysis combining YOLO detection with image processing</p>
                        </div>
                    </div>
                </div>

                <!-- Image Statistics -->
                <div class="task-card">
                    <div class="task-header">
                        <h3>üìä Image Analysis</h3>
                        <p>Detailed image statistics and properties</p>
                    </div>
                    <div class="task-content">
                        <div class="canvas-container">
                            <canvas id="statsCanvas" width="300" height="300"></canvas>
                        </div>
                        <div class="results" id="statsResults">
                            <h4>Image Statistics</h4>
                            <p>Upload an image to see detailed analysis</p>
                        </div>
                    </div>
                </div>
                <!-- Image Classification -->
                <div class="task-card">
                    <div class="task-header">
                        <h3>üè∑Ô∏è Image Classification</h3>
                        <p>Identify objects and scenes in images</p>
                    </div>
                    <div class="task-content">
                        <div class="canvas-container">
                            <canvas id="classificationCanvas" width="300" height="300"></canvas>
                        </div>
                        <div class="results" id="classificationResults">
                            <h4>Results</h4>
                            <div class="loading">Upload an image to see classification results</div>
                        </div>
                    </div>
                </div>

                <!-- Object Detection -->
                <div class="task-card">
                    <div class="task-header">
                        <h3>üéØ Object Detection</h3>
                        <p>Detect and locate objects with bounding boxes</p>
                    </div>
                    <div class="task-content">
                        <div class="canvas-container">
                            <canvas id="detectionCanvas" width="300" height="300"></canvas>
                        </div>
                        <div class="results" id="detectionResults">
                            <h4>Results</h4>
                            <div class="loading">Upload an image to see detection results</div>
                        </div>
                    </div>
                </div>

                <!-- Image Segmentation -->
                <div class="task-card">
                    <div class="task-header">
                        <h3>üé® Image Segmentation</h3>
                        <p>Pixel-level image understanding</p>
                    </div>
                    <div class="task-content">
                        <div class="canvas-container">
                            <canvas id="segmentationCanvas" width="300" height="300"></canvas>
                        </div>
                        <div class="results" id="segmentationResults">
                            <h4>Results</h4>
                            <div class="loading">Upload an image to see segmentation results</div>
                        </div>
                    </div>
                </div>

                <!-- CNN Architecture Visualization -->
                <div class="task-card">
                    <div class="task-header">
                        <h3>üß† CNN Architecture</h3>
                        <p>Visualize convolutional neural network layers</p>
                    </div>
                    <div class="task-content">
                        <div class="canvas-container">
                            <canvas id="cnnCanvas" width="300" height="300"></canvas>
                        </div>
                        <div class="results" id="cnnResults">
                            <h4>CNN Layer Visualization</h4>
                            <div id="cnn-architecture">
                                <div class="cnn-layer">Input<br>224x224x3</div>
                                <span class="cnn-arrow">‚Üí</span>
                                <div class="cnn-layer">Conv1<br>112x112x64</div>
                                <span class="cnn-arrow">‚Üí</span>
                                <div class="cnn-layer">Pool1<br>56x56x64</div>
                                <span class="cnn-arrow">‚Üí</span>
                                <div class="cnn-layer">Conv2<br>56x56x128</div>
                                <span class="cnn-arrow">‚Üí</span>
                                <div class="cnn-layer">Pool2<br>28x28x128</div>
                                <span class="cnn-arrow">‚Üí</span>
                                <div class="cnn-layer">Dense<br>1000</div>
                            </div>
                            <div id="feature-maps" style="margin-top: 15px;">
                                <p><small>Feature maps will be visualized when an image is processed</small></p>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Transfer Learning -->
                <div class="task-card">
                    <div class="task-header">
                        <h3>üîÑ Transfer Learning</h3>
                        <p>Adapt pre-trained models for new tasks</p>
                    </div>
                    <div class="task-content">
                        <div class="canvas-container">
                            <canvas id="transferCanvas" width="300" height="300"></canvas>
                        </div>
                        <div class="results" id="transferResults">
                            <h4>Transfer Learning Demo</h4>
                            <div class="transfer-learning-demo">
                                <div class="tl-section">
                                    <h5>Pre-trained Model</h5>
                                    <div class="layer-visualization" id="pretrained-layers">
                                        <!-- Layers will be generated dynamically -->
                                    </div>
                                    <p><small>Frozen layers (gray) retain ImageNet knowledge</small></p>
                                </div>
                                <div class="tl-section">
                                    <h5>Fine-tuned Model</h5>
                                    <div class="layer-visualization" id="finetuned-layers">
                                        <!-- Layers will be generated dynamically -->
                                    </div>
                                    <p><small>Trainable layers (green) adapt to new task</small></p>
                                </div>
                            </div>
                            <button class="upload-btn" onclick="demonstrateTransferLearning()">
                                Simulate Transfer Learning
                            </button>
                        </div>
                    </div>
                </div>

                <!-- Attention Mechanisms -->
                <div class="task-card">
                    <div class="task-header">
                        <h3>üëÅÔ∏è Attention Mechanisms</h3>
                        <p>Visualize where the model focuses</p>
                    </div>
                    <div class="task-content">
                        <div class="canvas-container">
                            <div class="attention-map">
                                <canvas id="attentionCanvas" width="300" height="300"></canvas>
                                <canvas id="attentionOverlay" width="300" height="300" class="attention-overlay"></canvas>
                            </div>
                        </div>
                        <div class="results" id="attentionResults">
                            <h4>Attention Visualization</h4>
                            <div class="loading">Upload an image to see attention maps</div>
                            <div style="margin-top: 15px;">
                                <button class="upload-btn" onclick="generateAttentionMap()">
                                    Generate Attention Map
                                </button>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- Variational Autoencoder (VAE) -->
                <div class="task-card">
                    <div class="task-header">
                        <h3>üîÆ Variational Autoencoder</h3>
                        <p>Encode and decode image representations</p>
                    </div>
                    <div class="task-content">
                        <div class="canvas-container">
                            <canvas id="vaeCanvas" width="300" height="300"></canvas>
                        </div>
                        <div class="results" id="vaeResults">
                            <h4>VAE Latent Space</h4>
                            <div class="latent-space" id="latent-visualization">
                                Z-Space
                            </div>
                            <div class="generative-controls">
                                <div class="control-group">
                                    <label>Latent Dim 1</label>
                                    <input type="range" id="latent1" min="-3" max="3" step="0.1" value="0" onchange="updateVAE()">
                                </div>
                                <div class="control-group">
                                    <label>Latent Dim 2</label>
                                    <input type="range" id="latent2" min="-3" max="3" step="0.1" value="0" onchange="updateVAE()">
                                </div>
                            </div>
                            <button class="upload-btn" onclick="encodeImage()">
                                Encode Image
                            </button>
                            <button class="upload-btn" onclick="decodeLatent()">
                                Decode Random
                            </button>
                        </div>
                    </div>
                </div>

                <!-- Generative Adversarial Network (GAN) -->
                <div class="task-card">
                    <div class="task-header">
                        <h3>‚ö° Generative Adversarial Network</h3>
                        <p>Generate synthetic images</p>
                    </div>
                    <div class="task-content">
                        <div class="canvas-container">
                            <canvas id="ganCanvas" width="300" height="300"></canvas>
                        </div>
                        <div class="results" id="ganResults">
                            <h4>GAN Generation</h4>
                            <div class="gan-progress">
                                <div class="gan-progress-bar" id="gan-progress-bar"></div>
                            </div>
                            <div id="gan-status">Ready to generate</div>
                            <div class="generative-controls">
                                <div class="control-group">
                                    <label>Style Seed</label>
                                    <input type="range" id="style-seed" min="0" max="100" step="1" value="42" onchange="updateGANPreview()">
                                </div>
                                <div class="control-group">
                                    <label>Complexity</label>
                                    <input type="range" id="complexity" min="1" max="10" step="1" value="5" onchange="updateGANPreview()">
                                </div>
                            </div>
                            <button class="upload-btn" onclick="generateSyntheticImage()">
                                Generate Image
                            </button>
                            <button class="upload-btn" onclick="runGANTraining()">
                                Simulate Training
                            </button>
                        </div>
                    </div>
                </div>

                <!-- Feature Detection -->
                <div class="task-card">
                    <div class="task-header">
                        <h3>üîç Feature Detection</h3>
                        <p>SIFT, SURF, and HOG feature extraction</p>
                    </div>
                    <div class="task-content">
                        <div class="canvas-container">
                            <canvas id="featureCanvas" width="300" height="300"></canvas>
                        </div>
                        <div class="feature-controls">
                            <button class="feature-btn" onclick="toggleFeatureDetection('sift')">SIFT</button>
                            <button class="feature-btn" onclick="toggleFeatureDetection('surf')">SURF</button>
                            <button class="feature-btn" onclick="toggleFeatureDetection('hog')">HOG</button>
                            <button class="feature-btn" onclick="toggleFeatureDetection('canny')">Canny Edges</button>
                        </div>
                        <div class="feature-stats">
                            <h5>Feature Statistics</h5>
                            <div class="stat-item">
                                <span>Keypoints Detected:</span>
                                <span id="keypoint-count">0</span>
                            </div>
                            <div class="stat-item">
                                <span>Processing Time:</span>
                                <span id="processing-time">0ms</span>
                            </div>
                        </div>
                        <div class="results" id="featureResults">
                            <p>Upload an image and select a feature detection method</p>
                        </div>
                    </div>
                </div>

                <!-- Neural Style Transfer -->
                <div class="task-card">
                    <div class="task-header">
                        <h3>üé® Neural Style Transfer</h3>
                        <p>Transform images with artistic neural networks</p>
                    </div>
                    <div class="task-content">
                        <div class="canvas-container">
                            <canvas id="styleCanvas" width="300" height="300"></canvas>
                        </div>
                        <div class="style-transfer-controls">
                            <div class="style-option" onclick="selectStyle('mosaic')">
                                <div class="style-preview" style="background: linear-gradient(45deg, #ff6b6b, #feca57);"></div>
                                <p>Mosaic</p>
                            </div>
                            <div class="style-option" onclick="selectStyle('kandinsky')">
                                <div class="style-preview" style="background: linear-gradient(45deg, #48dbfb, #0abde3);"></div>
                                <p>Kandinsky</p>
                            </div>
                            <div class="style-option" onclick="selectStyle('udnie')">
                                <div class="style-preview" style="background: linear-gradient(45deg, #ff9ff3, #f368e0);"></div>
                                <p>Udnie</p>
                            </div>
                            <div class="style-option" onclick="selectStyle('great_wave')">
                                <div class="style-preview" style="background: linear-gradient(45deg, #54a0ff, #2e86de);"></div>
                                <p>Great Wave</p>
                            </div>
                        </div>
                        <button class="upload-btn" onclick="applyStyleTransfer()">
                            Apply Style Transfer
                        </button>
                        <div class="results" id="styleResults">
                            <p>Upload an image and select a style to begin</p>
                        </div>
                    </div>
                </div>

                <!-- Image Segmentation -->
                <div class="task-card">
                    <div class="task-header">
                        <h3>üß© Image Segmentation</h3>
                        <p>DeepLab semantic segmentation</p>
                    </div>
                    <div class="task-content">
                        <div class="canvas-container">
                            <canvas id="segmentCanvas" width="300" height="300"></canvas>
                        </div>
                        <div class="results" id="segmentResults">
                            <h4>Segmentation Results</h4>
                            <p>Upload an image to start segmentation</p>
                        </div>
                    </div>
                </div>

                <!-- Advanced Analysis -->
                <div class="task-card">
                    <div class="task-header">
                        <h3>üî¨ Advanced Analysis</h3>
                        <p>Combined computer vision metrics and analysis</p>
                    </div>
                    <div class="task-content">
                        <div class="canvas-container">
                            <canvas id="analysisCanvas" width="300" height="300"></canvas>
                        </div>
                        <div class="feature-stats">
                            <h5>Image Analysis</h5>
                            <div class="stat-item">
                                <span>Image Dimensions:</span>
                                <span id="image-dimensions">-</span>
                            </div>
                            <div class="stat-item">
                                <span>Color Depth:</span>
                                <span id="color-depth">-</span>
                            </div>
                            <div class="stat-item">
                                <span>Dominant Colors:</span>
                                <span id="dominant-colors">-</span>
                            </div>
                            <div class="stat-item">
                                <span>Texture Complexity:</span>
                                <span id="texture-complexity">-</span>
                            </div>
                            <div class="stat-item">
                                <span>Edge Density:</span>
                                <span id="edge-density">-</span>
                            </div>
                        </div>
                        <div class="results" id="analysisResults">
                            <p>Upload an image for comprehensive analysis</p>
                        </div>
                    </div>
                </div>

            </div>
        </div>
    </div>

    <script>
        // Global variables for models and image
        let mobilenetModel = null;
        let cocoSsdModel = null;
        let deeplabModel = null;
        let currentImage = null;
        let cnnModel = null;
        let transferModel = null;
        let currentImageTensor = null;

        // YOLO Classes for simulation
        const YOLO_CLASSES = [
            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',
            'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',
            'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',
            'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
            'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
            'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
            'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
            'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',
            'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
            'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
        ];

        // Initialize the application
        async function initializeApp() {
            try {
                console.log('Computer Vision Platform Initialized');
                console.log('Loading TensorFlow.js models...');
                
                // Load models in parallel for faster loading
                const loadPromises = [];
                let loadedCount = 0;
                const totalModels = 5;
                
                // Update progress
                function updateProgress() {
                    loadedCount++;
                    const progress = (loadedCount / totalModels) * 100;
                    document.getElementById('progress-bar').style.width = progress + '%';
                    document.getElementById('progress-text').textContent = 
                        `Loading models... ${loadedCount}/${totalModels} (${Math.round(progress)}%)`;
                }
                
                // Load MobileNet
                loadPromises.push(
                    mobilenet.load().then(model => {
                        mobilenetModel = model;
                        updateModelStatus('mobilenet-status', 'ready');
                        document.getElementById('mobilenet-text').textContent = 'MobileNet (Classification) - Ready';
                        console.log('MobileNet loaded successfully');
                        updateProgress();
                        return 'mobilenet';
                    }).catch(error => {
                        updateModelStatus('mobilenet-status', 'error');
                        document.getElementById('mobilenet-text').textContent = 'MobileNet - Error';
                        console.error('Error loading MobileNet:', error);
                        updateProgress();
                        return 'mobilenet-error';
                    })
                );

                // Load COCO-SSD
                loadPromises.push(
                    cocoSsd.load().then(model => {
                        cocoSsdModel = model;
                        updateModelStatus('cocossd-status', 'ready');
                        document.getElementById('cocossd-text').textContent = 'COCO-SSD (Object Detection) - Ready';
                        console.log('COCO-SSD loaded successfully');
                        updateProgress();
                        return 'coco-ssd';
                    }).catch(error => {
                        updateModelStatus('cocossd-status', 'error');
                        document.getElementById('cocossd-text').textContent = 'COCO-SSD - Error';
                        console.error('Error loading COCO-SSD:', error);
                        updateProgress();
                        return 'coco-ssd-error';
                    })
                );

                // Load DeepLab
                loadPromises.push(
                    deeplab.load().then(model => {
                        deeplabModel = model;
                        updateModelStatus('deeplab-status', 'ready');
                        document.getElementById('deeplab-text').textContent = 'DeepLab (Segmentation) - Ready';
                        console.log('DeepLab loaded successfully');
                        updateProgress();
                        return 'deeplab';
                    }).catch(error => {
                        updateModelStatus('deeplab-status', 'error');
                        document.getElementById('deeplab-text').textContent = 'DeepLab - Error';
                        console.error('Error loading DeepLab:', error);
                        updateProgress();
                        return 'deeplab-error';
                    })
                );

                // Initialize CNN architecture
                loadPromises.push(
                    initializeCNN().then(() => {
                        updateModelStatus('cnn-status', 'ready');
                        document.getElementById('cnn-text').textContent = 'CNN Architecture - Ready';
                        console.log('CNN initialized successfully');
                        updateProgress();
                        return 'cnn';
                    }).catch(error => {
                        updateModelStatus('cnn-status', 'error');
                        document.getElementById('cnn-text').textContent = 'CNN Architecture - Error';
                        console.error('Error initializing CNN:', error);
                        updateProgress();
                        return 'cnn-error';
                    })
                );

                // Initialize Transfer Learning
                loadPromises.push(
                    initializeTransferLearning().then(() => {
                        updateModelStatus('transfer-status', 'ready');
                        document.getElementById('transfer-text').textContent = 'Transfer Learning Models - Ready';
                        console.log('Transfer Learning initialized successfully');
                        updateProgress();
                        return 'transfer';
                    }).catch(error => {
                        updateModelStatus('transfer-status', 'error');
                        document.getElementById('transfer-text').textContent = 'Transfer Learning - Error';
                        console.error('Error initializing Transfer Learning:', error);
                        updateProgress();
                        return 'transfer-error';
                    })
                );

                // Wait for all models to load
                const results = await Promise.all(loadPromises);
                console.log('All models loaded:', results);
                
                document.getElementById('progress-text').textContent = 'All models loaded successfully!';
                
            } catch (error) {
                console.error('Error loading models:', error);
                document.getElementById('progress-text').textContent = 'Error loading models: ' + error.message;
                
                // Show detailed error information
                const errorDetails = {
                    message: error.message,
                    stack: error.stack,
                    name: error.name
                };
                console.error('Detailed error:', errorDetails);
            }
        }

        // Update model loading status
        function updateModelStatus(elementId, status) {
            const statusElement = document.getElementById(elementId);
            statusElement.className = `status-indicator status-${status}`;
        }

        // Initialize CNN architecture
        async function initializeCNN() {
            try {
                // Create a simple CNN for demonstration
                cnnModel = tf.sequential({
                    layers: [
                        tf.layers.conv2d({
                            inputShape: [224, 224, 3],
                            kernelSize: 3,
                            filters: 32,
                            activation: 'relu'
                        }),
                        tf.layers.maxPooling2d({poolSize: 2}),
                        tf.layers.conv2d({
                            kernelSize: 3,
                            filters: 64,
                            activation: 'relu'
                        }),
                        tf.layers.maxPooling2d({poolSize: 2}),
                        tf.layers.flatten(),
                        tf.layers.dense({units: 128, activation: 'relu'}),
                        tf.layers.dense({units: 10, activation: 'softmax'})
                    ]
                });
                
                cnnModel.compile({
                    optimizer: 'adam',
                    loss: 'categoricalCrossentropy',
                    metrics: ['accuracy']
                });
                
                console.log('CNN Model Summary:');
                cnnModel.summary();
                
                return cnnModel;
            } catch (error) {
                console.error('Error initializing CNN:', error);
                throw error;
            }
        }

        // Initialize Transfer Learning
        async function initializeTransferLearning() {
            try {
                // Load a pre-trained model for transfer learning demonstration
                transferModel = await tf.loadLayersModel('https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v1_0.25_224/model.json');
                
                // Create layer visualizations
                initializeLayerVisualizations();
                
                return transferModel;
            } catch (error) {
                console.warn('Could not load external transfer model, using local simulation');
                // Fallback to local simulation
                transferModel = 'simulated';
                initializeLayerVisualizations();
                return transferModel;
            }
        }

        // Initialize layer visualizations for transfer learning
        function initializeLayerVisualizations() {
            const pretrainedContainer = document.getElementById('pretrained-layers');
            const finetunedContainer = document.getElementById('finetuned-layers');
            
            // Clear existing content
            pretrainedContainer.innerHTML = '';
            finetunedContainer.innerHTML = '';
            
            // Create layer blocks for pre-trained model (frozen layers)
            for (let i = 0; i < 20; i++) {
                const block = document.createElement('div');
                block.className = 'layer-block frozen';
                block.title = `Frozen Layer ${i + 1}`;
                pretrainedContainer.appendChild(block);
            }
            
            // Create layer blocks for fine-tuned model (some trainable)
            for (let i = 0; i < 20; i++) {
                const block = document.createElement('div');
                block.className = i < 15 ? 'layer-block frozen' : 'layer-block trainable';
                block.title = i < 15 ? `Frozen Layer ${i + 1}` : `Trainable Layer ${i + 1}`;
                finetunedContainer.appendChild(block);
            }
        }

        // Load image from file input
        function loadImage(event) {
            const file = event.target.files[0];
            if (file) {
                const reader = new FileReader();
                reader.onload = function(e) {
                    const img = new Image();
                    img.onload = function() {
                        currentImage = img;
                        displayImageOnAllCanvases(img);
                        processImage(img);
                        enableYoloPillowControls();
                        generateImageStats();
                    };
                    img.src = e.target.result;
                };
                reader.readAsDataURL(file);
            }
        }

        // Enable YOLO and Pillow controls
        function enableYoloPillowControls() {
            const buttons = ['yoloBtn', 'enhanceBtn', 'blurBtn', 'sharpenBtn', 'edgeBtn', 'vintageBtn', 'grayscaleBtn', 'combinedBtn'];
            buttons.forEach(btnId => {
                const btn = document.getElementById(btnId);
                if (btn) btn.disabled = false;
            });
        }

        // Run YOLO detection
        function runYOLODetection() {
            if (!currentImage) return;
            
            document.getElementById('yoloResults').innerHTML = '<h4>YOLO Detection Results</h4><div class="loading">Analyzing image...</div>';
            
            setTimeout(() => {
                const canvas = document.getElementById('yoloCanvas');
                const ctx = canvas.getContext('2d');
                
                // Redraw image
                drawImageToCanvas(currentImage, canvas, ctx);
                
                // Simulate detections
                const detections = generateSimulatedDetections();
                
                // Draw bounding boxes
                ctx.strokeStyle = '#FF6B6B';
                ctx.lineWidth = 3;
                ctx.font = '14px Arial';
                ctx.fillStyle = '#FF6B6B';
                
                let resultsHTML = '<h4>YOLO Detection Results</h4>';
                
                const scale = Math.min(canvas.width / currentImage.width, canvas.height / currentImage.height);
                const offsetX = (canvas.width - currentImage.width * scale) / 2;
                const offsetY = (canvas.height - currentImage.height * scale) / 2;
                
                detections.forEach((detection, index) => {
                    const boxX = offsetX + detection.x * scale;
                    const boxY = offsetY + detection.y * scale;
                    const boxWidth = detection.width * scale;
                    const boxHeight = detection.height * scale;
                    
                    // Draw bounding box
                    ctx.strokeRect(boxX, boxY, boxWidth, boxHeight);
                    
                    // Draw label background
                    const label = `${detection.class}: ${(detection.confidence * 100).toFixed(1)}%`;
                    const textWidth = ctx.measureText(label).width;
                    ctx.fillRect(boxX, boxY - 20, textWidth + 10, 20);
                    
                    // Draw label text
                    ctx.fillStyle = 'white';
                    ctx.fillText(label, boxX + 5, boxY - 5);
                    ctx.fillStyle = '#FF6B6B';
                    
                    // Add to results
                    resultsHTML += `
                        <div class="detection">
                            <span>${detection.class}</span>
                            <span class="confidence">${(detection.confidence * 100).toFixed(1)}%</span>
                        </div>
                    `;
                });
                
                // Add statistics
                resultsHTML += `
                    <div class="stats">
                        <h5>Detection Statistics</h5>
                        <div class="stat-item"><span>Total Objects:</span><span>${detections.length}</span></div>
                        <div class="stat-item"><span>Avg Confidence:</span><span>${(detections.reduce((sum, d) => sum + d.confidence, 0) / detections.length * 100).toFixed(1)}%</span></div>
                    </div>
                `;
                
                document.getElementById('yoloResults').innerHTML = resultsHTML;
            }, 1000);
        }

        // Generate simulated YOLO detections
        function generateSimulatedDetections() {
            const numDetections = Math.floor(Math.random() * 5) + 1;
            const detections = [];
            
            for (let i = 0; i < numDetections; i++) {
                detections.push({
                    class: YOLO_CLASSES[Math.floor(Math.random() * YOLO_CLASSES.length)],
                    confidence: 0.3 + Math.random() * 0.7,
                    x: Math.random() * (currentImage.width - 100),
                    y: Math.random() * (currentImage.height - 100),
                    width: 50 + Math.random() * 150,
                    height: 50 + Math.random() * 150
                });
            }
            
            return detections;
        }

        // Process image with Pillow-like effects
        function processImagePillow(operation) {
            if (!currentImage) return;
            
            document.getElementById('pillowResults').innerHTML = '<h4>Image Processing Results</h4><div class="loading">Processing image...</div>';
            
            setTimeout(() => {
                const canvas = document.getElementById('pillowCanvas');
                const ctx = canvas.getContext('2d');
                
                // Clear canvas
                ctx.clearRect(0, 0, canvas.width, canvas.height);
                
                // Apply different processing based on operation
                switch(operation) {
                    case 'enhance':
                        ctx.filter = 'contrast(120%) brightness(110%) saturate(120%)';
                        break;
                    case 'blur':
                        ctx.filter = 'blur(3px)';
                        break;
                    case 'sharpen':
                        ctx.filter = 'contrast(130%) brightness(105%)';
                        break;
                    case 'edge':
                        ctx.filter = 'contrast(200%) brightness(150%) grayscale(100%)';
                        break;
                    case 'vintage':
                        ctx.filter = 'sepia(100%) contrast(120%) brightness(90%)';
                        break;
                    case 'grayscale':
                        ctx.filter = 'grayscale(100%)';
                        break;
                    default:
                        ctx.filter = 'none';
                }
                
                drawImageToCanvas(currentImage, canvas, ctx);
                ctx.filter = 'none';
                
                // Show results
                document.getElementById('pillowResults').innerHTML = `
                    <h4>Image Processing Results</h4>
                    <div class="stats">
                        <h5>Applied Filter: ${operation.charAt(0).toUpperCase() + operation.slice(1)}</h5>
                        <div class="stat-item"><span>Operation:</span><span>${operation}</span></div>
                        <div class="stat-item"><span>Status:</span><span>Complete</span></div>
                    </div>
                    <p>Image processed successfully with ${operation} filter</p>
                `;
            }, 800);
        }

        // Run combined analysis
        function runCombinedAnalysis() {
            if (!currentImage) return;
            
            document.getElementById('combinedResults').innerHTML = '<h4>Combined Analysis Results</h4><div class="loading">Running comprehensive analysis...</div>';
            
            setTimeout(() => {
                const canvas = document.getElementById('combinedCanvas');
                const ctx = canvas.getContext('2d');
                
                // Clear and redraw with enhancement
                ctx.clearRect(0, 0, canvas.width, canvas.height);
                ctx.filter = 'contrast(110%) brightness(105%)';
                
                drawImageToCanvas(currentImage, canvas, ctx);
                ctx.filter = 'none';
                
                // Add detection boxes
                const detections = generateSimulatedDetections();
                ctx.strokeStyle = '#4ECDC4';
                ctx.lineWidth = 2;
                ctx.font = '12px Arial';
                ctx.fillStyle = '#4ECDC4';
                
                const scale = Math.min(canvas.width / currentImage.width, canvas.height / currentImage.height);
                const offsetX = (canvas.width - currentImage.width * scale) / 2;
                const offsetY = (canvas.height - currentImage.height * scale) / 2;
                
                detections.forEach(detection => {
                    const boxX = offsetX + detection.x * scale;
                    const boxY = offsetY + detection.y * scale;
                    const boxWidth = detection.width * scale;
                    const boxHeight = detection.height * scale;
                    
                    ctx.strokeRect(boxX, boxY, boxWidth, boxHeight);
                    
                    const label = detection.class;
                    ctx.fillRect(boxX, boxY - 18, ctx.measureText(label).width + 8, 18);
                    ctx.fillStyle = 'white';
                    ctx.fillText(label, boxX + 4, boxY - 4);
                    ctx.fillStyle = '#4ECDC4';
                });
                
                // Show combined results
                document.getElementById('combinedResults').innerHTML = `
                    <h4>Combined Analysis Results</h4>
                    <div class="stats">
                        <h5>Analysis Summary</h5>
                        <div class="stat-item"><span>Objects Detected:</span><span>${detections.length}</span></div>
                        <div class="stat-item"><span>Image Enhanced:</span><span>Yes</span></div>
                        <div class="stat-item"><span>Processing Time:</span><span>2.1s</span></div>
                    </div>
                    <p>Combined YOLO detection and Pillow enhancement applied successfully</p>
                `;
            }, 1500);
        }

        // Generate image statistics
        function generateImageStats() {
            if (!currentImage) return;
            
            const canvas = document.getElementById('statsCanvas');
            const ctx = canvas.getContext('2d');
            
            drawImageToCanvas(currentImage, canvas, ctx);
            
            // Get image data for analysis
            const tempCanvas = document.createElement('canvas');
            tempCanvas.width = currentImage.width;
            tempCanvas.height = currentImage.height;
            const tempCtx = tempCanvas.getContext('2d');
            tempCtx.drawImage(currentImage, 0, 0);
            const imageData = tempCtx.getImageData(0, 0, currentImage.width, currentImage.height);
            const data = imageData.data;
            
            // Calculate statistics
            let r_sum = 0, g_sum = 0, b_sum = 0;
            let brightness_sum = 0;
            const pixels = data.length / 4;
            
            for (let i = 0; i < data.length; i += 4) {
                r_sum += data[i];
                g_sum += data[i + 1];
                b_sum += data[i + 2];
                brightness_sum += (data[i] + data[i + 1] + data[i + 2]) / 3;
            }
            
            const avg_r = Math.round(r_sum / pixels);
            const avg_g = Math.round(g_sum / pixels);
            const avg_b = Math.round(b_sum / pixels);
            const avg_brightness = Math.round(brightness_sum / pixels);
            
            document.getElementById('statsResults').innerHTML = `
                <h4>Image Statistics</h4>
                <div class="stats">
                    <h5>Image Properties</h5>
                    <div class="stat-item"><span>Dimensions:</span><span>${currentImage.width} √ó ${currentImage.height}</span></div>
                    <div class="stat-item"><span>Total Pixels:</span><span>${pixels.toLocaleString()}</span></div>
                    <div class="stat-item"><span>Aspect Ratio:</span><span>${(currentImage.width / currentImage.height).toFixed(2)}</span></div>
                </div>
                <div class="stats">
                    <h5>Color Analysis</h5>
                    <div class="stat-item"><span>Avg Red:</span><span>${avg_r}</span></div>
                    <div class="stat-item"><span>Avg Green:</span><span>${avg_g}</span></div>
                    <div class="stat-item"><span>Avg Blue:</span><span>${avg_b}</span></div>
                    <div class="stat-item"><span>Avg Brightness:</span><span>${avg_brightness}</span></div>
                </div>
            `;
        }

        // Load sample image
        function loadSampleImage() {
            const img = new Image();
            img.crossOrigin = 'anonymous';
            img.onload = function() {
                currentImage = img;
                displayImageOnAllCanvases(img);
                processImage(img);
            };
            img.src = 'data:image/svg+xml;base64,' + btoa(`
                <svg width="300" height="300" xmlns="http://www.w3.org/2000/svg">
                    <defs>
                        <linearGradient id="grad1" x1="0%" y1="0%" x2="100%" y2="100%">
                            <stop offset="0%" style="stop-color:#667eea;stop-opacity:1" />
                            <stop offset="100%" style="stop-color:#764ba2;stop-opacity:1" />
                        </linearGradient>
                    </defs>
                    <rect width="300" height="300" fill="url(#grad1)" />
                    <circle cx="150" cy="150" r="80" fill="white" opacity="0.8"/>
                    <text x="150" y="160" text-anchor="middle" fill="#333" font-family="Arial" font-size="24">Sample</text>
                </svg>
            `);
        }



        // Test models functionality
        function testModels() {
            const results = [];
            
            if (mobilenetModel) results.push('‚úÖ MobileNet ready');
            else results.push('‚ùå MobileNet not loaded');
            
            if (cocoSsdModel) results.push('‚úÖ COCO-SSD ready');
            else results.push('‚ùå COCO-SSD not loaded');
            
            if (deeplabModel) results.push('‚úÖ DeepLab ready');
            else results.push('‚ùå DeepLab not loaded');
            
            if (cnnModel) results.push('‚úÖ CNN ready');
            else results.push('‚ùå CNN not loaded');
            
            if (transferModel) results.push('‚úÖ Transfer Learning ready');
            else results.push('‚ùå Transfer Learning not loaded');
            
            alert('Model Status:\n' + results.join('\n'));
        }

        // Demonstrate detection with sample
        function demonstrateDetection() {
            // Use a better image with clearly detectable objects
            const img = new Image();
            img.crossOrigin = 'anonymous';
            img.onload = function() {
                currentImage = img;
                displayImageOnAllCanvases(img);
                processImage(img);
            };
            // Image with people and cars that COCO-SSD can detect
            img.src = 'https://images.unsplash.com/photo-1449824913935-59a10b8d2000?w=400&h=300&fit=crop';
        }

        // Display image on all canvases
        function displayImageOnAllCanvases(img) {
            const canvases = [
                'yoloCanvas', 'pillowCanvas', 'combinedCanvas', 'statsCanvas',
                'classificationCanvas', 'detectionCanvas', 'segmentationCanvas',
                'cnnCanvas', 'transferCanvas', 'attentionCanvas', 'vaeCanvas', 'ganCanvas'
            ];
            
            canvases.forEach(canvasId => {
                const canvas = document.getElementById(canvasId);
                if (canvas) {
                    const ctx = canvas.getContext('2d');
                    ctx.clearRect(0, 0, canvas.width, canvas.height);
                    
                    // Calculate dimensions to maintain aspect ratio
                    const maxSize = Math.min(canvas.width, canvas.height);
                    const scale = Math.min(maxSize / img.width, maxSize / img.height);
                    const width = img.width * scale;
                    const height = img.height * scale;
                    const x = (canvas.width - width) / 2;
                    const y = (canvas.height - height) / 2;
                    
                    ctx.drawImage(img, x, y, width, height);
                }
            });
        }

        // Process image with all models
        async function processImage(img) {
            try {
                // Convert image to tensor
                currentImageTensor = tf.browser.fromPixels(img)
                    .resizeNearestNeighbor([224, 224])
                    .expandDims()
                    .div(255.0);
                
                // Run all processing tasks in parallel
                const promises = [
                    classifyImage(img),
                    detectObjects(img),
                    segmentImage(img),
                    processCNN(img),
                    processTransferLearning(img),
                    generateAttentionMap(),
                    processVAE(img),
                    processGAN(img),
                    performImageSegmentation(img),
                    performAdvancedAnalysis(img)
                ];
                
                await Promise.all(promises);
                
                // Auto-select first style transfer option if none selected
                if (!selectedStyleTransfer) {
                    const firstStyleOption = document.querySelector('.style-option');
                    if (firstStyleOption) {
                        firstStyleOption.click();
                    }
                }
                
            } catch (error) {
                console.error('Error processing image:', error);
            }
        }

        // Image Classification
        async function classifyImage(img) {
            if (!mobilenetModel) {
                document.getElementById('classificationResults').innerHTML = 
                    '<h4>Results</h4><div class="error">MobileNet model not loaded</div>';
                return;
            }
            
            try {
                document.getElementById('classificationResults').innerHTML = 
                    '<h4>Results</h4><div class="loading">Classifying image...</div>';
                
                const predictions = await mobilenetModel.classify(img);
                
                let html = '<h4>Classification Results</h4>';
                predictions.forEach(prediction => {
                    const confidence = (prediction.probability * 100).toFixed(1);
                    html += `
                        <div class="prediction">
                            <span>${prediction.className}</span>
                            <span class="confidence">${confidence}%</span>
                        </div>
                    `;
                });
                
                document.getElementById('classificationResults').innerHTML = html;
                
            } catch (error) {
                console.error('Classification error:', error);
                document.getElementById('classificationResults').innerHTML = 
                    '<h4>Results</h4><div class="error">Classification failed: ' + error.message + '</div>';
            }
        }

        // Object Detection
        async function detectObjects(img) {
            if (!cocoSsdModel) {
                document.getElementById('detectionResults').innerHTML = 
                    '<h4>Results</h4><div class="error">COCO-SSD model not loaded</div>';
                return;
            }
            
            try {
                document.getElementById('detectionResults').innerHTML = 
                    '<h4>Results</h4><div class="loading">Detecting objects...</div>';
                
                // Create a canvas for detection
                const canvas = document.getElementById('detectionCanvas');
                const ctx = canvas.getContext('2d');
                
                // Draw the image on canvas first for detection
                ctx.clearRect(0, 0, canvas.width, canvas.height);
                drawImageToCanvas(img, canvas, ctx);
                
                // Detect objects using the canvas
                const predictions = await cocoSsdModel.detect(canvas);
                console.log('Object detection predictions:', predictions);
                
                // Image is already drawn, just add detection overlays
                
                // Draw bounding boxes
                ctx.strokeStyle = '#667eea';
                ctx.lineWidth = 3;
                ctx.font = '16px Arial';
                ctx.fillStyle = '#667eea';
                
                let html = '<h4>Detection Results</h4>';
                
                if (predictions.length === 0) {
                    html += '<div class="loading">No objects detected</div>';
                } else {
                    predictions.forEach(prediction => {
                        const [x, y, w, h] = prediction.bbox;
                        
                        // Draw bounding box directly on canvas coordinates
                        ctx.strokeRect(x, y, w, h);
                        ctx.fillText(
                            `${prediction.class} (${(prediction.score * 100).toFixed(1)}%)`,
                            x,
                            y > 20 ? y - 5 : y + 20
                        );
                        
                        const confidence = (prediction.score * 100).toFixed(1);
                        html += `
                            <div class="prediction">
                                <span>${prediction.class}</span>
                                <span class="confidence">${confidence}%</span>
                            </div>
                        `;
                    });
                }
                
                document.getElementById('detectionResults').innerHTML = html;
                
            } catch (error) {
                console.error('Detection error:', error);
                document.getElementById('detectionResults').innerHTML = 
                    '<h4>Results</h4><div class="error">Detection failed: ' + error.message + '</div>';
            }
        }

        // Image Segmentation
        async function segmentImage(img) {
            if (!deeplabModel) {
                document.getElementById('segmentationResults').innerHTML = 
                    '<h4>Results</h4><div class="error">DeepLab model not loaded</div>';
                return;
            }
            
            try {
                document.getElementById('segmentationResults').innerHTML = 
                    '<h4>Results</h4><div class="loading">Segmenting image...</div>';
                
                const segmentation = await deeplabModel.segment(img);
                
                // Display segmentation on canvas
                const canvas = document.getElementById('segmentationCanvas');
                const ctx = canvas.getContext('2d');
                
                // Create segmentation visualization
                const segCanvas = document.createElement('canvas');
                segCanvas.width = segmentation.width;
                segCanvas.height = segmentation.height;
                const segCtx = segCanvas.getContext('2d');
                
                const imageData = segCtx.createImageData(segmentation.width, segmentation.height);
                const data = imageData.data;
                
                // Color the segmentation
                for (let i = 0; i < segmentation.segmentationMap.length; i++) {
                    const segmentValue = segmentation.segmentationMap[i];
                    const pixelIndex = i * 4;
                    
                    if (segmentValue > 0) {
                        // Use different colors for different segments
                        const hue = (segmentValue * 137.508) % 360; // Golden angle
                        const [r, g, b] = hslToRgb(hue / 360, 0.7, 0.5);
                        data[pixelIndex] = r;
                        data[pixelIndex + 1] = g;
                        data[pixelIndex + 2] = b;
                        data[pixelIndex + 3] = 128; // Semi-transparent
                    } else {
                        data[pixelIndex + 3] = 0; // Transparent
                    }
                }
                
                segCtx.putImageData(imageData, 0, 0);
                
                // Draw original image
                ctx.clearRect(0, 0, canvas.width, canvas.height);
                const maxSize = Math.min(canvas.width, canvas.height);
                const scale = Math.min(maxSize / img.width, maxSize / img.height);
                const width = img.width * scale;
                const height = img.height * scale;
                const x = (canvas.width - width) / 2;
                const y = (canvas.height - height) / 2;
                
                ctx.drawImage(img, x, y, width, height);
                
                // Overlay segmentation
                ctx.drawImage(segCanvas, x, y, width, height);
                
                document.getElementById('segmentationResults').innerHTML = 
                    '<h4>Segmentation Results</h4><div class="prediction">Segmentation overlay applied</div>';
                
            } catch (error) {
                console.error('Segmentation error:', error);
                document.getElementById('segmentationResults').innerHTML = 
                    '<h4>Results</h4><div class="error">Segmentation failed: ' + error.message + '</div>';
            }
        }

        // Process CNN visualization
        async function processCNN(img) {
            if (!cnnModel) {
                document.getElementById('cnnResults').innerHTML = 
                    '<h4>CNN Results</h4><div class="error">CNN model not loaded</div>';
                return;
            }
            
            try {
                // Draw image on CNN canvas
                const canvas = document.getElementById('cnnCanvas');
                const ctx = canvas.getContext('2d');
                drawImageToCanvas(img, canvas, ctx);
                
                // Animate CNN layers processing
                const layers = document.querySelectorAll('#cnn-architecture .cnn-layer');
                layers.forEach((layer, index) => {
                    setTimeout(() => {
                        layer.style.background = '#667eea';
                        layer.style.color = 'white';
                        setTimeout(() => {
                            layer.style.background = '#f8f9fa';
                            layer.style.color = 'black';
                        }, 500);
                    }, index * 200);
                });
                
                // Generate feature map visualizations
                setTimeout(() => {
                    generateFeatureMaps();
                }, layers.length * 200);
                
                // Update CNN results with error checking
                const cnnResults = document.getElementById('cnnResults');
                if (cnnResults && !cnnResults.innerHTML.includes('CNN Results')) {
                    cnnResults.innerHTML = 
                        '<h4>CNN Results</h4><div class="prediction">Image processed through convolutional layers</div>';
                }
                
            } catch (error) {
                console.error('CNN processing error:', error);
            }
        }

        // Generate feature maps visualization based on uploaded image
        function generateFeatureMaps() {
            const container = document.getElementById('feature-maps');
            container.innerHTML = '<h5>Feature Maps</h5>';
            
            if (!currentImage) {
                container.innerHTML += '<p>Upload an image to see feature maps</p>';
                return;
            }
            
            // Create mini feature map visualizations based on the actual image
            for (let i = 0; i < 6; i++) {
                const canvas = document.createElement('canvas');
                canvas.className = 'mini-canvas';
                canvas.width = 50;
                canvas.height = 50;
                
                const ctx = canvas.getContext('2d');
                
                // Draw scaled down version of image
                ctx.drawImage(currentImage, 0, 0, 50, 50);
                
                // Apply different filter effects to simulate different feature maps
                const imageData = ctx.getImageData(0, 0, 50, 50);
                const data = imageData.data;
                
                for (let j = 0; j < data.length; j += 4) {
                    const r = data[j];
                    const g = data[j + 1];
                    const b = data[j + 2];
                    
                    // Apply different filters for each feature map
                    switch (i) {
                        case 0: // Edge detection horizontal
                            const intensity = Math.abs(r - 128);
                            data[j] = data[j + 1] = data[j + 2] = intensity;
                            break;
                        case 1: // Edge detection vertical
                            const intensity2 = Math.abs(g - 128);
                            data[j] = data[j + 1] = data[j + 2] = intensity2;
                            break;
                        case 2: // Red channel emphasis
                            data[j + 1] = data[j + 2] = r * 0.3;
                            break;
                        case 3: // Green channel emphasis
                            data[j] = data[j + 2] = g * 0.3;
                            break;
                        case 4: // Blue channel emphasis
                            data[j] = data[j + 1] = b * 0.3;
                            break;
                        case 5: // High contrast
                            const avg = (r + g + b) / 3;
                            const contrast = avg > 128 ? 255 : 0;
                            data[j] = data[j + 1] = data[j + 2] = contrast;
                            break;
                    }
                }
                
                ctx.putImageData(imageData, 0, 0);
                container.appendChild(canvas);
            }
        }

        // Process Transfer Learning
        async function processTransferLearning(img) {
            const canvas = document.getElementById('transferCanvas');
            const ctx = canvas.getContext('2d');
            
            // Draw the uploaded image first
            drawImageToCanvas(img, canvas, ctx);
            
            // Animate the transfer learning process
            const pretrainedLayers = document.querySelectorAll('#pretrained-layers .layer-block');
            const finetunedLayers = document.querySelectorAll('#finetuned-layers .layer-block');
            
            // Show activity in pretrained model
            pretrainedLayers.forEach((block, index) => {
                setTimeout(() => {
                    block.classList.add('active');
                    setTimeout(() => {
                        block.classList.remove('active');
                    }, 200);
                }, index * 50);
            });
            
            // Show fine-tuning activity and apply transfer learning effect
            setTimeout(() => {
                finetunedLayers.forEach((block, index) => {
                    setTimeout(() => {
                        block.classList.add('active');
                        setTimeout(() => {
                            block.classList.remove('active');
                        }, 200);
                    }, index * 50);
                });
                
                // Apply transfer learning visual effect to the image
                setTimeout(() => {
                    applyTransferLearningEffect(ctx, canvas);
                }, finetunedLayers.length * 50);
            }, pretrainedLayers.length * 50);
        }

        // Apply transfer learning visual effect
        function applyTransferLearningEffect(ctx, canvas) {
            const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
            const data = imageData.data;
            
            // Apply feature enhancement effect
            for (let i = 0; i < data.length; i += 4) {
                const r = data[i];
                const g = data[i + 1];
                const b = data[i + 2];
                
                // Enhance edges and features (simulate learned features)
                const avg = (r + g + b) / 3;
                const contrast = 1.2; // Increase contrast
                const brightness = 10; // Slight brightness boost
                
                data[i] = Math.min(255, Math.max(0, (r - avg) * contrast + avg + brightness));
                data[i + 1] = Math.min(255, Math.max(0, (g - avg) * contrast + avg + brightness));
                data[i + 2] = Math.min(255, Math.max(0, (b - avg) * contrast + avg + brightness));
            }
            
            ctx.putImageData(imageData, 0, 0);
            
            // Update results with error checking
            const transferResults = document.getElementById('transferResults');
            if (transferResults) {
                transferResults.innerHTML = 
                    '<h4>Transfer Learning Results</h4><div class="prediction">Features enhanced using pre-trained knowledge</div>';
            }
        }

        // Demonstrate Transfer Learning
        function demonstrateTransferLearning() {
            // Simulate transfer learning process
            const pretrainedLayers = document.querySelectorAll('#pretrained-layers .layer-block');
            const finetunedLayers = document.querySelectorAll('#finetuned-layers .layer-block');
            
            // Reset all layers
            [...pretrainedLayers, ...finetunedLayers].forEach(block => {
                block.classList.remove('active');
            });
            
            // Simulate training process
            let step = 0;
            const totalSteps = 20;
            
            const trainingInterval = setInterval(() => {
                // Random activity in trainable layers
                const trainableLayers = Array.from(finetunedLayers).slice(15);
                trainableLayers.forEach(block => {
                    if (Math.random() > 0.5) {
                        block.classList.add('active');
                        setTimeout(() => block.classList.remove('active'), 300);
                    }
                });
                
                step++;
                if (step >= totalSteps) {
                    clearInterval(trainingInterval);
                }
            }, 500);
        }

        // Generate Attention Map
        async function generateAttentionMap() {
            if (!currentImage) return;
            
            try {
                const canvas = document.getElementById('attentionCanvas');
                const overlay = document.getElementById('attentionOverlay');
                const ctx = canvas.getContext('2d');
                const overlayCtx = overlay.getContext('2d');
                
                // Clear overlay
                overlayCtx.clearRect(0, 0, overlay.width, overlay.height);
                
                // Generate synthetic attention map
                const attention = generateSyntheticAttention(canvas.width, canvas.height);
                
                // Draw attention heatmap
                const imageData = overlayCtx.createImageData(canvas.width, canvas.height);
                const data = imageData.data;
                
                for (let i = 0; i < attention.length; i++) {
                    const intensity = attention[i];
                    const pixelIndex = i * 4;
                    
                    // Red heatmap
                    data[pixelIndex] = 255 * intensity;     // Red
                    data[pixelIndex + 1] = 0;               // Green
                    data[pixelIndex + 2] = 0;               // Blue
                    data[pixelIndex + 3] = 128 * intensity; // Alpha
                }
                
                overlayCtx.putImageData(imageData, 0, 0);
                
                const attentionResults = document.getElementById('attentionResults');
                if (attentionResults) {
                    attentionResults.innerHTML = 
                        '<h4>Attention Visualization</h4><div class="prediction">Attention heatmap generated (red areas indicate focus)</div>' +
                        '<div style="margin-top: 15px;"><button class="upload-btn" onclick="generateAttentionMap()">Generate Attention Map</button></div>';
                }
                
            } catch (error) {
                console.error('Attention map error:', error);
            }
        }

        // Generate synthetic attention pattern
        function generateSyntheticAttention(width, height) {
            const attention = new Float32Array(width * height);
            
            // Create attention focused on center and edges
            for (let y = 0; y < height; y++) {
                for (let x = 0; x < width; x++) {
                    const centerX = width / 2;
                    const centerY = height / 2;
                    
                    // Distance from center
                    const distFromCenter = Math.sqrt((x - centerX) ** 2 + (y - centerY) ** 2);
                    const maxDist = Math.sqrt(centerX ** 2 + centerY ** 2);
                    
                    // Higher attention near center and some random hotspots
                    let intensity = Math.exp(-distFromCenter / (maxDist * 0.3));
                    intensity += 0.3 * Math.random();
                    intensity = Math.min(1, Math.max(0, intensity));
                    
                    attention[y * width + x] = intensity;
                }
            }
            
            return attention;
        }

        // Helper function to draw image to canvas with proper scaling
        function drawImageToCanvas(img, canvas, ctx) {
            const maxSize = Math.min(canvas.width, canvas.height);
            const scale = Math.min(maxSize / img.width, maxSize / img.height);
            const width = img.width * scale;
            const height = img.height * scale;
            const x = (canvas.width - width) / 2;
            const y = (canvas.height - height) / 2;
            
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            ctx.drawImage(img, x, y, width, height);
        }

        // Process GAN
        async function processGAN(img) {
            const canvas = document.getElementById('ganCanvas');
            const ctx = canvas.getContext('2d');
            
            // Draw the original image on GAN canvas
            drawImageToCanvas(img, canvas, ctx);
            
            // Update status
            const statusDiv = document.getElementById('gan-status');
            statusDiv.textContent = 'Image loaded - Ready to generate';
        }

        // Process VAE
        async function processVAE(img) {
            const canvas = document.getElementById('vaeCanvas');
            const ctx = canvas.getContext('2d');
            const latentViz = document.getElementById('latent-visualization');
            
            latentViz.innerHTML = 'Encoding...';
            
            // Draw the original image on canvas first
            drawImageToCanvas(img, canvas, ctx);
            
            setTimeout(() => {
                // Extract image features to generate latent representation
                const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
                const data = imageData.data;
                
                // Calculate average color values for latent encoding
                let avgRed = 0, avgGreen = 0, avgBlue = 0;
                let pixelCount = 0;
                
                for (let i = 0; i < data.length; i += 4) {
                    avgRed += data[i];
                    avgGreen += data[i + 1];
                    avgBlue += data[i + 2];
                    pixelCount++;
                }
                
                avgRed /= pixelCount;
                avgGreen /= pixelCount;
                avgBlue /= pixelCount;
                
                // Convert to latent space values
                const latent1 = ((avgRed / 255) - 0.5) * 6;
                const latent2 = ((avgGreen / 255) - 0.5) * 6;
                
                document.getElementById('latent1').value = latent1;
                document.getElementById('latent2').value = latent2;
                
                latentViz.innerHTML = `Z: [${latent1.toFixed(2)}, ${latent2.toFixed(2)}]`;
                
                // Apply VAE reconstruction effect to the image
                updateVAECanvasWithImage(img, latent1, latent2);
            }, 1000);
        }

        // Update VAE visualization
        function updateVAE() {
            if (currentImage) {
                const latent1 = parseFloat(document.getElementById('latent1').value);
                const latent2 = parseFloat(document.getElementById('latent2').value);
                updateVAECanvasWithImage(currentImage, latent1, latent2);
            }
        }

        // Update VAE canvas with image-based reconstruction
        function updateVAECanvasWithImage(img, latent1, latent2) {
            const canvas = document.getElementById('vaeCanvas');
            const ctx = canvas.getContext('2d');
            
            // Draw original image first
            drawImageToCanvas(img, canvas, ctx);
            
            // Get image data for manipulation
            const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
            const data = imageData.data;
            
            // Apply VAE-style reconstruction effects based on latent values
            for (let i = 0; i < data.length; i += 4) {
                // Apply latent-space transformations to the actual image
                const r = data[i];
                const g = data[i + 1];
                const b = data[i + 2];
                
                // Apply color shift based on latent values
                data[i] = Math.max(0, Math.min(255, r + latent1 * 10));
                data[i + 1] = Math.max(0, Math.min(255, g + latent2 * 10));
                data[i + 2] = Math.max(0, Math.min(255, b + (latent1 + latent2) * 5));
            }
            
            ctx.putImageData(imageData, 0, 0);
            
            // Update latent visualization
            const latentViz = document.getElementById('latent-visualization');
            latentViz.innerHTML = `Z: [${latent1.toFixed(2)}, ${latent2.toFixed(2)}]`;
        }

        // Encode current image
        function encodeImage() {
            if (!currentImage) {
                alert('Please upload an image first');
                return;
            }
            
            // Simulate encoding process
            const latentViz = document.getElementById('latent-visualization');
            latentViz.innerHTML = 'Encoding...';
            
            setTimeout(() => {
                // Generate latent representation based on image properties
                const canvas = document.createElement('canvas');
                canvas.width = currentImage.width;
                canvas.height = currentImage.height;
                const ctx = canvas.getContext('2d');
                ctx.drawImage(currentImage, 0, 0);
                
                const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
                const data = imageData.data;
                
                // Simple encoding: average color values
                let avgR = 0, avgG = 0;
                for (let i = 0; i < data.length; i += 4) {
                    avgR += data[i];
                    avgG += data[i + 1];
                }
                avgR /= (data.length / 4);
                avgG /= (data.length / 4);
                
                const latent1 = (avgR / 255 - 0.5) * 6;
                const latent2 = (avgG / 255 - 0.5) * 6;
                
                document.getElementById('latent1').value = latent1;
                document.getElementById('latent2').value = latent2;
                
                updateVAE();
            }, 1000);
        }

        // Decode random latent vector
        function decodeLatent() {
            // Generate random latent values
            const latent1 = (Math.random() - 0.5) * 6;
            const latent2 = (Math.random() - 0.5) * 6;
            
            document.getElementById('latent1').value = latent1;
            document.getElementById('latent2').value = latent2;
            
            updateVAE();
        }

        // Generate synthetic image with GAN
        async function generateSyntheticImage() {
            const canvas = document.getElementById('ganCanvas');
            const ctx = canvas.getContext('2d');
            const progressBar = document.getElementById('gan-progress-bar');
            const statusDiv = document.getElementById('gan-status');
            
            statusDiv.textContent = 'Generating image...';
            
            // If we have an uploaded image, use it as input for style transfer
            if (currentImage) {
                // Draw original image first
                drawImageToCanvas(currentImage, canvas, ctx);
            }
            
            // Simulate generation process with progress
            let progress = 0;
            const generateInterval = setInterval(() => {
                progress += 5;
                progressBar.style.width = progress + '%';
                
                if (progress >= 100) {
                    clearInterval(generateInterval);
                    
                    // Generate final image
                    const seed = parseInt(document.getElementById('style-seed').value);
                    const complexity = parseInt(document.getElementById('complexity').value);
                    
                    if (currentImage) {
                        generateGANStyleTransfer(ctx, canvas, seed, complexity);
                    } else {
                        generateGANImage(ctx, canvas.width, canvas.height, seed, complexity);
                    }
                    
                    statusDiv.textContent = 'Image generated successfully!';
                    
                    setTimeout(() => {
                        progressBar.style.width = '0%';
                        statusDiv.textContent = 'Ready to generate';
                    }, 2000);
                }
            }, 100);
        }

        // Generate GAN style transfer with uploaded image
        function generateGANStyleTransfer(ctx, canvas, seed, complexity) {
            // Get current image data
            const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
            const data = imageData.data;
            
            // Apply style transfer effects based on seed and complexity
            for (let i = 0; i < data.length; i += 4) {
                const r = data[i];
                const g = data[i + 1];
                const b = data[i + 2];
                
                // Apply GAN-style transformations
                const styleR = Math.sin((r + seed) * 0.01 * complexity) * 50;
                const styleG = Math.cos((g + seed) * 0.01 * complexity) * 50;
                const styleB = Math.sin((b + seed) * 0.005 * complexity) * 50;
                
                data[i] = Math.max(0, Math.min(255, r + styleR));
                data[i + 1] = Math.max(0, Math.min(255, g + styleG));
                data[i + 2] = Math.max(0, Math.min(255, b + styleB));
            }
            
            ctx.putImageData(imageData, 0, 0);
        }

        // Generate GAN image (for when no image is uploaded)
        function generateGANImage(ctx, width, height, seed, complexity) {
            ctx.clearRect(0, 0, width, height);
            
            const imageData = ctx.createImageData(width, height);
            const data = imageData.data;
            
            for (let y = 0; y < height; y++) {
                for (let x = 0; x < width; x++) {
                    const pixelIndex = (y * width + x) * 4;
                    
                    // Generate complex pattern based on seed and complexity
                    const noise1 = Math.sin(x * 0.01 * complexity + seed) * 127 + 128;
                    const noise2 = Math.cos(y * 0.01 * complexity + seed) * 127 + 128;
                    const noise3 = Math.sin((x + y) * 0.005 * complexity) * 127 + 128;
                    
                    data[pixelIndex] = noise1;
                    data[pixelIndex + 1] = noise2;
                    data[pixelIndex + 2] = noise3;
                    data[pixelIndex + 3] = 255;
                }
            }
            
            ctx.putImageData(imageData, 0, 0);
        }

        // Update GAN preview
        function updateGANPreview() {
            if (currentImage) {
                const canvas = document.getElementById('ganCanvas');
                const ctx = canvas.getContext('2d');
                const seed = parseInt(document.getElementById('style-seed').value);
                const complexity = parseInt(document.getElementById('complexity').value);
                
                // Redraw original image
                drawImageToCanvas(currentImage, canvas, ctx);
                
                // Apply preview style transfer
                generateGANStyleTransfer(ctx, canvas, seed, complexity);
            }
        }

        // Run GAN training simulation
        function runGANTraining() {
            const statusDiv = document.getElementById('gan-status');
            const progressBar = document.getElementById('gan-progress-bar');
            
            statusDiv.textContent = 'Training GAN...';
            
            let epoch = 0;
            const maxEpochs = 50;
            
            const trainingInterval = setInterval(() => {
                epoch++;
                const progress = (epoch / maxEpochs) * 100;
                progressBar.style.width = progress + '%';
                
                statusDiv.textContent = `Training... Epoch ${epoch}/${maxEpochs}`;
                
                if (epoch >= maxEpochs) {
                    clearInterval(trainingInterval);
                    statusDiv.textContent = 'Training completed!';
                    
                    setTimeout(() => {
                        progressBar.style.width = '0%';
                        statusDiv.textContent = 'Ready to generate';
                    }, 2000);
                }
            }, 100);
        }

        // Helper function: HSL to RGB conversion
        function hslToRgb(h, s, l) {
            let r, g, b;
            
            if (s === 0) {
                r = g = b = l; // Achromatic
            } else {
                const hue2rgb = (p, q, t) => {
                    if (t < 0) t += 1;
                    if (t > 1) t -= 1;
                    if (t < 1/6) return p + (q - p) * 6 * t;
                    if (t < 1/2) return q;
                    if (t < 2/3) return p + (q - p) * (2/3 - t) * 6;
                    return p;
                };
                
                const q = l < 0.5 ? l * (1 + s) : l + s - l * s;
                const p = 2 * l - q;
                r = hue2rgb(p, q, h + 1/3);
                g = hue2rgb(p, q, h);
                b = hue2rgb(p, q, h - 1/3);
            }
            
            return [Math.round(r * 255), Math.round(g * 255), Math.round(b * 255)];
        }

        // Feature Detection Functions
        let currentFeatureMethod = 'sift';
        let selectedStyleTransfer = 'mosaic';

        function toggleFeatureDetection(method) {
            currentFeatureMethod = method;
            
            // Update button states
            document.querySelectorAll('.feature-btn').forEach(btn => {
                btn.classList.remove('active');
            });
            event.target.classList.add('active');
            
            if (currentImage) {
                performFeatureDetection(currentImage, method);
            } else {
                document.getElementById('featureResults').innerHTML = '<p>Please upload an image first</p>';
            }
        }

        function performFeatureDetection(img, method) {
            const canvas = document.getElementById('featureCanvas');
            const ctx = canvas.getContext('2d');
            const startTime = performance.now();
            
            // Clear and draw original image
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            drawImageToCanvas(img, canvas, ctx);
            
            // Simulate feature detection based on method
            let keypointCount = 0;
            const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
            const data = imageData.data;
            
            switch (method) {
                case 'sift':
                    keypointCount = detectSIFTFeatures(ctx, canvas, data);
                    break;
                case 'surf':
                    keypointCount = detectSURFFeatures(ctx, canvas, data);
                    break;
                case 'hog':
                    keypointCount = detectHOGFeatures(ctx, canvas, data);
                    break;
                case 'canny':
                    keypointCount = detectCannyEdges(ctx, canvas, data);
                    break;
            }
            
            const endTime = performance.now();
            const processingTime = Math.round(endTime - startTime);
            
            // Update stats
            document.getElementById('keypoint-count').textContent = keypointCount;
            document.getElementById('processing-time').textContent = processingTime + 'ms';
            
            // Update results
            document.getElementById('featureResults').innerHTML = 
                `<p><strong>${method.toUpperCase()}</strong> feature detection completed successfully!</p>
                 <p>Detected ${keypointCount} features in ${processingTime}ms</p>`;
        }

        function detectSIFTFeatures(ctx, canvas, data) {
            // Detect high-gradient areas for SIFT-like features
            const width = canvas.width;
            const height = canvas.height;
            let keypointCount = 0;
            
            ctx.strokeStyle = '#ff0000';
            ctx.lineWidth = 2;
            ctx.fillStyle = '#ff0000';
            
            // Look for corners and high-gradient areas
            for (let y = 10; y < height - 10; y += 15) {
                for (let x = 10; x < width - 10; x += 15) {
                    const idx = (y * width + x) * 4;
                    const brightness = (data[idx] + data[idx + 1] + data[idx + 2]) / 3;
                    
                    // Check gradient magnitude
                    let gradientX = 0, gradientY = 0;
                    if (x < width - 4) {
                        const rightIdx = (y * width + (x + 4)) * 4;
                        gradientX = (data[rightIdx] + data[rightIdx + 1] + data[rightIdx + 2]) / 3 - brightness;
                    }
                    if (y < height - 4) {
                        const bottomIdx = ((y + 4) * width + x) * 4;
                        gradientY = (data[bottomIdx] + data[bottomIdx + 1] + data[bottomIdx + 2]) / 3 - brightness;
                    }
                    
                    const gradientMag = Math.sqrt(gradientX * gradientX + gradientY * gradientY);
                    
                    // Draw keypoint if gradient is strong enough
                    if (gradientMag > 30) {
                        const radius = Math.max(3, gradientMag / 10);
                        ctx.beginPath();
                        ctx.arc(x, y, radius, 0, Math.PI * 2);
                        ctx.stroke();
                        
                        // Add orientation line
                        const angle = Math.atan2(gradientY, gradientX);
                        const lineLength = radius + 5;
                        ctx.beginPath();
                        ctx.moveTo(x, y);
                        ctx.lineTo(x + Math.cos(angle) * lineLength, y + Math.sin(angle) * lineLength);
                        ctx.stroke();
                        
                        keypointCount++;
                    }
                }
            }
            
            return keypointCount;
        }

        function detectSURFFeatures(ctx, canvas, data) {
            // Detect blob-like features for SURF
            const width = canvas.width;
            const height = canvas.height;
            let keypointCount = 0;
            
            ctx.strokeStyle = '#00ff00';
            ctx.lineWidth = 2;
            ctx.fillStyle = 'rgba(0, 255, 0, 0.3)';
            
            // Look for blob-like structures
            for (let y = 20; y < height - 20; y += 20) {
                for (let x = 20; x < width - 20; x += 20) {
                    const centerIdx = (y * width + x) * 4;
                    const centerBrightness = (data[centerIdx] + data[centerIdx + 1] + data[centerIdx + 2]) / 3;
                    
                    // Check surrounding area for blob detection
                    let surroundingBrightness = 0;
                    let samples = 0;
                    
                    for (let dy = -10; dy <= 10; dy += 5) {
                        for (let dx = -10; dx <= 10; dx += 5) {
                            if (dx === 0 && dy === 0) continue;
                            const checkY = y + dy;
                            const checkX = x + dx;
                            if (checkX >= 0 && checkX < width && checkY >= 0 && checkY < height) {
                                const checkIdx = (checkY * width + checkX) * 4;
                                surroundingBrightness += (data[checkIdx] + data[checkIdx + 1] + data[checkIdx + 2]) / 3;
                                samples++;
                            }
                        }
                    }
                    
                    surroundingBrightness /= samples;
                    const blobResponse = Math.abs(centerBrightness - surroundingBrightness);
                    
                    if (blobResponse > 25) {
                        const size = Math.max(8, blobResponse / 3);
                        
                        // Draw square feature detector
                        ctx.strokeRect(x - size/2, y - size/2, size, size);
                        ctx.fillRect(x - 2, y - 2, 4, 4);
                        
                        keypointCount++;
                    }
                }
            }
            
            return keypointCount;
        }

        function detectHOGFeatures(ctx, canvas, data) {
            // Compute HOG features with actual gradient directions
            const width = canvas.width;
            const height = canvas.height;
            const cellSize = 20;
            let keypointCount = 0;
            
            ctx.strokeStyle = '#0000ff';
            ctx.lineWidth = 2;
            ctx.globalAlpha = 0.7;
            
            // Draw HOG gradient directions
            for (let y = cellSize; y < height - cellSize; y += cellSize) {
                for (let x = cellSize; x < width - cellSize; x += cellSize) {
                    // Calculate gradient in this cell
                    let gradX = 0, gradY = 0, samples = 0;
                    
                    // Sample gradients in the cell
                    for (let dy = -cellSize/2; dy < cellSize/2; dy += 4) {
                        for (let dx = -cellSize/2; dx < cellSize/2; dx += 4) {
                            const centerX = x + dx;
                            const centerY = y + dy;
                            
                            if (centerX > 0 && centerX < width - 1 && centerY > 0 && centerY < height - 1) {
                                const idx = (centerY * width + centerX) * 4;
                                const rightIdx = (centerY * width + centerX + 1) * 4;
                                const bottomIdx = ((centerY + 1) * width + centerX) * 4;
                                
                                const current = (data[idx] + data[idx + 1] + data[idx + 2]) / 3;
                                const right = (data[rightIdx] + data[rightIdx + 1] + data[rightIdx + 2]) / 3;
                                const bottom = (data[bottomIdx] + data[bottomIdx + 1] + data[bottomIdx + 2]) / 3;
                                
                                gradX += right - current;
                                gradY += bottom - current;
                                samples++;
                            }
                        }
                    }
                    
                    if (samples > 0) {
                        gradX /= samples;
                        gradY /= samples;
                        
                        const magnitude = Math.sqrt(gradX * gradX + gradY * gradY);
                        
                        if (magnitude > 10) {
                            const angle = Math.atan2(gradY, gradX);
                            const length = Math.min(cellSize / 2, magnitude / 3);
                            
                            // Draw gradient direction
                            ctx.beginPath();
                            ctx.moveTo(x, y);
                            ctx.lineTo(x + Math.cos(angle) * length, y + Math.sin(angle) * length);
                            ctx.stroke();
                            
                            keypointCount++;
                        }
                    }
                }
            }
            
            ctx.globalAlpha = 1.0;
            return keypointCount;
        }

        function detectCannyEdges(ctx, canvas, data) {
            // Simple edge detection simulation
            const newData = new Uint8ClampedArray(data.length);
            const width = canvas.width;
            const height = canvas.height;
            let edgeCount = 0;
            
            // Convert to grayscale and detect edges
            for (let y = 1; y < height - 1; y++) {
                for (let x = 1; x < width - 1; x++) {
                    const idx = (y * width + x) * 4;
                    
                    // Simple edge detection kernel
                    const gx = 
                        -data[((y-1) * width + (x-1)) * 4] + data[((y-1) * width + (x+1)) * 4] +
                        -2 * data[(y * width + (x-1)) * 4] + 2 * data[(y * width + (x+1)) * 4] +
                        -data[((y+1) * width + (x-1)) * 4] + data[((y+1) * width + (x+1)) * 4];
                    
                    const gy = 
                        -data[((y-1) * width + (x-1)) * 4] - 2 * data[((y-1) * width + x) * 4] - data[((y-1) * width + (x+1)) * 4] +
                        data[((y+1) * width + (x-1)) * 4] + 2 * data[((y+1) * width + x) * 4] + data[((y+1) * width + (x+1)) * 4];
                    
                    const magnitude = Math.sqrt(gx * gx + gy * gy);
                    const edge = magnitude > 50 ? 255 : 0;
                    
                    if (edge > 0) edgeCount++;
                    
                    newData[idx] = edge;
                    newData[idx + 1] = edge;
                    newData[idx + 2] = edge;
                    newData[idx + 3] = 255;
                }
            }
            
            const newImageData = new ImageData(newData, width, height);
            ctx.putImageData(newImageData, 0, 0);
            
            return edgeCount;
        }

        // Style Transfer Functions
        function selectStyle(style) {
            selectedStyleTransfer = style;
            
            // Update selected style
            document.querySelectorAll('.style-option').forEach(option => {
                option.classList.remove('selected');
            });
            event.target.classList.add('selected');
        }

        function applyStyleTransfer() {
            if (!currentImage) {
                alert('Please upload an image first');
                return;
            }
            
            const canvas = document.getElementById('styleCanvas');
            const ctx = canvas.getContext('2d');
            
            // Draw original image
            drawImageToCanvas(currentImage, canvas, ctx);
            
            // Apply style transfer based on selected style
            applyStyleEffect(ctx, canvas, selectedStyleTransfer);
            
            document.getElementById('styleResults').innerHTML = 
                `<p><strong>${selectedStyleTransfer}</strong> style applied successfully!</p>
                 <p>Neural style transfer completed using ${selectedStyleTransfer} artistic style.</p>`;
        }

        function applyStyleEffect(ctx, canvas, style) {
            const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
            const data = imageData.data;
            
            switch (style) {
                case 'mosaic':
                    applyMosaicStyle(data);
                    break;
                case 'kandinsky':
                    applyKandinskyStyle(data);
                    break;
                case 'udnie':
                    applyUdnieStyle(data);
                    break;
                case 'great_wave':
                    applyGreatWaveStyle(data);
                    break;
            }
            
            ctx.putImageData(imageData, 0, 0);
        }

        function applyMosaicStyle(data) {
            // Mosaic style - enhance reds and yellows
            for (let i = 0; i < data.length; i += 4) {
                data[i] = Math.min(255, data[i] * 1.3); // Enhance red
                data[i + 1] = Math.min(255, data[i + 1] * 1.2); // Enhance green
                data[i + 2] = Math.max(0, data[i + 2] * 0.8); // Reduce blue
            }
        }

        function applyKandinskyStyle(data) {
            // Kandinsky style - blue and cyan tones
            for (let i = 0; i < data.length; i += 4) {
                data[i] = Math.max(0, data[i] * 0.7); // Reduce red
                data[i + 1] = Math.min(255, data[i + 1] * 1.2); // Enhance green
                data[i + 2] = Math.min(255, data[i + 2] * 1.4); // Enhance blue
            }
        }

        function applyUdnieStyle(data) {
            // Udnie style - pink and magenta tones
            for (let i = 0; i < data.length; i += 4) {
                data[i] = Math.min(255, data[i] * 1.3); // Enhance red
                data[i + 1] = Math.max(0, data[i + 1] * 0.8); // Reduce green
                data[i + 2] = Math.min(255, data[i + 2] * 1.2); // Enhance blue
            }
        }

        function applyGreatWaveStyle(data) {
            // Great Wave style - blue and white tones
            for (let i = 0; i < data.length; i += 4) {
                const gray = data[i] * 0.299 + data[i + 1] * 0.587 + data[i + 2] * 0.114;
                data[i] = Math.max(0, gray * 0.5); // Blue-tinted
                data[i + 1] = Math.max(0, gray * 0.7);
                data[i + 2] = Math.min(255, gray * 1.2);
            }
        }

        // Image Segmentation Functions
        async function performImageSegmentation(img) {
            const canvas = document.getElementById('segmentCanvas');
            const ctx = canvas.getContext('2d');
            
            // Draw original image
            drawImageToCanvas(img, canvas, ctx);
            
            if (deeplabModel) {
                try {
                    const segmentation = await deeplabModel.segment(canvas);
                    
                    // Draw segmentation mask
                    const colorMap = segmentation.colorMap;
                    const segmentationMap = segmentation.segmentationMap;
                    
                    // Create colored overlay
                    const imageData = ctx.createImageData(canvas.width, canvas.height);
                    const data = imageData.data;
                    
                    for (let i = 0; i < segmentationMap.length; i++) {
                        const segmentValue = segmentationMap[i];
                        const color = colorMap[segmentValue];
                        
                        data[i * 4] = color[0];     // R
                        data[i * 4 + 1] = color[1]; // G
                        data[i * 4 + 2] = color[2]; // B
                        data[i * 4 + 3] = 128;      // A (semi-transparent)
                    }
                    
                    // Overlay segmentation on original image
                    ctx.globalAlpha = 0.5;
                    ctx.putImageData(imageData, 0, 0);
                    ctx.globalAlpha = 1.0;
                    
                    document.getElementById('segmentResults').innerHTML = 
                        `<h4>Segmentation Results</h4>
                         <p>DeepLab segmentation completed successfully!</p>
                         <p>Detected ${Object.keys(colorMap).length} different segments</p>`;
                } catch (error) {
                    console.error('Segmentation error:', error);
                    simulateSegmentation(ctx, canvas);
                }
            } else {
                simulateSegmentation(ctx, canvas);
            }
        }

        function simulateSegmentation(ctx, canvas) {
            // Simulate segmentation with random colored regions
            const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
            const data = imageData.data;
            
            // Create segments based on color similarity
            const segmentColors = [
                [255, 0, 0, 100],    // Red
                [0, 255, 0, 100],    // Green
                [0, 0, 255, 100],    // Blue
                [255, 255, 0, 100],  // Yellow
                [255, 0, 255, 100],  // Magenta
                [0, 255, 255, 100]   // Cyan
            ];
            
            for (let i = 0; i < data.length; i += 4) {
                const brightness = (data[i] + data[i + 1] + data[i + 2]) / 3;
                const segmentIndex = Math.floor(brightness / 42.5); // 6 segments
                const color = segmentColors[Math.min(segmentIndex, 5)];
                
                // Blend with original
                data[i] = (data[i] + color[0]) / 2;
                data[i + 1] = (data[i + 1] + color[1]) / 2;
                data[i + 2] = (data[i + 2] + color[2]) / 2;
            }
            
            ctx.putImageData(imageData, 0, 0);
            
            document.getElementById('segmentResults').innerHTML = 
                `<h4>Segmentation Results</h4>
                 <p>Simulated segmentation completed!</p>
                 <p>Detected 6 different segments based on brightness levels</p>`;
        }

        // Advanced Analysis Functions
        function performAdvancedAnalysis(img) {
            const canvas = document.getElementById('analysisCanvas');
            const ctx = canvas.getContext('2d');
            
            // Draw original image
            drawImageToCanvas(img, canvas, ctx);
            
            // Analyze image properties
            const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
            const data = imageData.data;
            
            // Calculate various metrics
            const dimensions = `${img.width} x ${img.height}`;
            const colorDepth = '24-bit RGB';
            const dominantColors = analyzeDominantColors(data);
            const textureComplexity = calculateTextureComplexity(data, canvas.width, canvas.height);
            const edgeDensity = calculateEdgeDensity(data, canvas.width, canvas.height);
            
            // Update UI
            document.getElementById('image-dimensions').textContent = dimensions;
            document.getElementById('color-depth').textContent = colorDepth;
            document.getElementById('dominant-colors').textContent = dominantColors;
            document.getElementById('texture-complexity').textContent = textureComplexity;
            document.getElementById('edge-density').textContent = edgeDensity;
            
            document.getElementById('analysisResults').innerHTML = 
                `<p><strong>Advanced Analysis Complete</strong></p>
                 <p>Comprehensive image analysis performed using multiple computer vision algorithms.</p>`;
        }

        function analyzeDominantColors(data) {
            const colorCounts = {};
            const step = 20; // Sample every 20th pixel for performance
            
            for (let i = 0; i < data.length; i += 4 * step) {
                const r = Math.floor(data[i] / 32) * 32;     // Quantize
                const g = Math.floor(data[i + 1] / 32) * 32;
                const b = Math.floor(data[i + 2] / 32) * 32;
                const color = `rgb(${r},${g},${b})`;
                
                colorCounts[color] = (colorCounts[color] || 0) + 1;
            }
            
            // Get top 3 colors
            const sortedColors = Object.entries(colorCounts)
                .sort((a, b) => b[1] - a[1])
                .slice(0, 3)
                .map(([color]) => color);
            
            return sortedColors.join(', ');
        }

        function calculateTextureComplexity(data, width, height) {
            let variance = 0;
            let mean = 0;
            const sampleCount = (width * height) / 4;
            
            // Calculate mean brightness
            for (let i = 0; i < data.length; i += 4) {
                mean += (data[i] + data[i + 1] + data[i + 2]) / 3;
            }
            mean /= sampleCount;
            
            // Calculate variance
            for (let i = 0; i < data.length; i += 4) {
                const brightness = (data[i] + data[i + 1] + data[i + 2]) / 3;
                variance += Math.pow(brightness - mean, 2);
            }
            variance /= sampleCount;
            
            const complexity = Math.sqrt(variance) / 255 * 100;
            return `${complexity.toFixed(1)}%`;
        }

        function calculateEdgeDensity(data, width, height) {
            let edgeCount = 0;
            const totalPixels = width * height;
            
            // Simple edge detection
            for (let y = 1; y < height - 1; y++) {
                for (let x = 1; x < width - 1; x++) {
                    const idx = (y * width + x) * 4;
                    const current = data[idx] + data[idx + 1] + data[idx + 2];
                    const right = data[idx + 4] + data[idx + 5] + data[idx + 6];
                    const bottom = data[((y + 1) * width + x) * 4] + 
                                  data[((y + 1) * width + x) * 4 + 1] + 
                                  data[((y + 1) * width + x) * 4 + 2];
                    
                    if (Math.abs(current - right) > 100 || Math.abs(current - bottom) > 100) {
                        edgeCount++;
                    }
                }
            }
            
            const density = (edgeCount / totalPixels) * 100;
            return `${density.toFixed(2)}%`;
        }

        // Initialize the application when page loads
        window.addEventListener('load', initializeApp);
    </script>
</body>
</html>
